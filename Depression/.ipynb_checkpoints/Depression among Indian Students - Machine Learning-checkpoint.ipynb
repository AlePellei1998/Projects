{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Depression using Classification Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This machine learning analysis purports to shed light on the following questions:\n",
    "- what **key factors** affect depression?\n",
    "- what students need the **most help**? \n",
    "\n",
    "A clear understanding of these two would allow us to **design policies** to better prevent depression and tackle its associated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#importing data analysis and visualisation packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 8) \n",
    "plt.rcParams['font.size'] = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing machine learning packages\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, cross_val_predict\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"depression_after_eda.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to check which column in the dataset contains string values\n",
    "\n",
    "df.applymap(type).eq(str).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features Degrees and Cities contain a lot of unique values, making the model too complex and also adding little explanatory power.\n",
    "\n",
    "print(df[\"Degree\"].nunique())\n",
    "print(df[\"City\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for now, I exclude City and Degree from our X features. The selected features for X are 11.\n",
    "\n",
    "X = df[[\"Gender\",\"Age\",\"Academic Pressure\",\"CGPA\",\"Study Satisfaction\", \"Sleep Duration\",\"Dietary Habits\",\"Suicidal Thoughts\", \"Work_study_hours\",\"Financial Stress\", \"Family History\"]]\n",
    "y = df[\"Depression\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking imbalances: the two classes depression=0 and depression=1 do not seem imbalanced.\n",
    "\n",
    "print(y.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One-hot encoding**\n",
    "\n",
    "Each category of a categorical feature is converted into a binary column (1/0) indicating whether the observation belongs to that category, allowing the model to understand the categorical data numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#one-hot encoding on the categorical variables: Gender, Sleep Duration, Dietary Habits, Suicidal Thoughts, Family History\n",
    "\n",
    "X = pd.get_dummies(columns=[\"Gender\", \"Sleep Duration\", \"Dietary Habits\", \"Suicidal Thoughts\", \"Family History\"], drop_first=True, data=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st and most used hold-out technique: train/test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=52, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) K-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN (K-Nearest Neighbors) is a simple, non-parametric machine learning algorithm used for classification. It predicts the label of a new observation based on the majority class of its k nearest neighbors in the training data.\n",
    "\n",
    "I define the following pipeline with StandardScaler and KNeighborsClassifier. StandardScaler standardizes our data by subtracting the mean from each feature and dividing by its standard deviation. Scaling features is important given that Euclidean distance is used to compute the number of KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I create a pipeline that scales the data. \n",
    "#the following happens under the hood automatically: \n",
    "# StandardScaler().fit(X_train)*: learns the mean and std from the training data\n",
    "# StandardScaler().transform(X_train)*: scales X_train using those parameters\n",
    "# KNeighborsClassifier().fit(...)*: trains on the scaled X_train\n",
    "\n",
    "pipeline = Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuning the model to understand when the testing error is minimised\n",
    "\n",
    "k_range=list(range(1,101))\n",
    "training_error=[]\n",
    "testing_error=[]\n",
    "\n",
    "for k in k_range:\n",
    "    pipeline = Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier(k))])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_class = pipeline.predict(X_test)\n",
    "    testing_accuracy=metrics.accuracy_score(y_test, y_pred_class)\n",
    "    testing_error.append(1-testing_accuracy)\n",
    "    \n",
    "    y_pred_class = pipeline.predict(X_train)\n",
    "    training_accuracy=metrics.accuracy_score(y_train, y_pred_class)\n",
    "    training_error.append(1-training_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I compare the testing and training error to see which value of k minimises the testing errors.\n",
    "\n",
    "knn_error = pd.DataFrame(list(zip(k_range, training_error, testing_error)), columns=[\"k\",\"training_error\",\"testing_error\"])\n",
    "knn_error.set_index(\"k\").sort_values(by=\"testing_error\", ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of nearest neighbours that minimise the testing error is k=88."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_error.set_index(\"k\").sort_values(by=\"testing_error\", ascending=True).plot();\n",
    "plt.savefig(\"knn_errors.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing the model accuracy with k=88\n",
    "\n",
    "pipeline_knn = Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier(88))])\n",
    "pipeline_knn.fit(X_train, y_train)\n",
    "y_pred_knn=pipeline_knn.predict(X_test)\n",
    "print(f'The accuracy of the KNN model is {metrics.accuracy_score(y_test, y_pred_knn):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy of the null model (most frequent class)\n",
    "#alternatively: y_test.value_counts(normalize=True)\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "dummy_clf.predict(X_test)\n",
    "print(f'The accuracy of the null model is: {dummy_clf.score(X_test, y_test):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the accuracy of the simple KNN is higher than the accuracy of the null model, our model has a better performance. Our model predicts correctly 85% of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Confusion matrix to investigate TP, TN, FP, FN\n",
    "\n",
    "class_names = ['depressed_no', 'depressed_yes']\n",
    "ConfusionMatrixDisplay.from_estimator(pipeline.fit(X_train, y_train), X_test, y_test,\n",
    "                                 display_labels=class_names, cmap=plt.cm.Blues);\n",
    "plt.savefig(\"Confusion matrix from KNN\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key metrics for classification**\n",
    "- **accuracy**: fraction of correct predictions overall;\n",
    "- **precision**: fraction of positive predictions that are correct (TP/TP+FP)\n",
    "- **recall (or sensitivity)**: fraction of actual positives that are correctly predicted (TP/TP+FN)\n",
    "- **specificity**: fraction of actual negatives that are correctly predicted (TN/TN+FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#classification report to investigate key metrics\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ROC-AUC curve**\n",
    "- ROC: t’s a curve that shows how well your model distinguishes between positive and negative classes at every possible threshold. When a model gives probabilities (like 0.8 or 0.2), you can choose any cutoff (say 0.5) to decide what’s “positive.” Each point on the curve corresponds to a different decision threshold. Low threshold → almost everything predicted as positive (high recall, high FPR). High threshold → almost nothing predicted as positive (low recall, low FPR)\n",
    "- AUC condenses the curve into a single number between 0 and 1, representing how well the model separates the two classes overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RocCurveDisplay.from_estimator(pipeline_knn, X_test, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is very good at ranking the positive probability scores, for depressed, higher than the negative cases (e.g. “not depressed”). In 92% of all random positive–negative pairs, the model gives the positive sample a higher score than the negative one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test[\"y_pred_knn\"] = y_pred_knn\n",
    "X_test[\"y\"]=df.loc[df.index, \"Depression\"]\n",
    "X_test[\"y_pred=y\"] = X_test[\"y_pred_knn\"] == X_test[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model's accuracy corresponds to checking when y=y_pred\n",
    "\n",
    "X_test[\"y_pred=y\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#investigating false negatives\n",
    "\n",
    "false_negatives_knn=X_test[(X_test[\"y_pred_knn\"]==0)&(X_test[\"y\"]==1)]\n",
    "display(false_negatives_knn.describe())\n",
    "display(false_negatives_knn.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#investigating true positives\n",
    "\n",
    "true_positives = X_test[(X_test[\"y_pred_knn\"]==1)&(X_test[\"y\"]==1)]\n",
    "display(true_positives.describe())\n",
    "display(true_positives.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparing TP and FN, we can see that false negatives, with respect to true positives, have a higher average age, lower average academic pressure, higher average study satisfaction, lower work study hours and lower financial stress. Because of this, the model incorrectly classified 334 observations that had features that do not normally associate with depression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#investigating false positives\n",
    "\n",
    "false_positives_knn = X_test[(X_test[\"y_pred_knn\"]==1)&(X_test[\"y\"]==0)]\n",
    "display(false_positives_knn.describe())\n",
    "display(false_positives_knn.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#investigating true negatives\n",
    "\n",
    "true_negatives_knn = X_test[(X_test[\"y_pred_knn\"]==0)&(X_test[\"y\"]==0)]\n",
    "display(true_negatives_knn.describe())\n",
    "display(true_negatives_knn.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparing FP and TN, we can see that FP, with respect to TN, have a lower average age, higher average academic pressure, lower average study satisfaction, higher average work study hours, a higher financial stress. Because of this, the model incorrectly classified 786 observations that are not depressed but that have traits similar to depressed students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visually comparing TP and FN\n",
    "\n",
    "columns = [\"Age\", \"Academic Pressure\", \"CGPA\", \"Study Satisfaction\", \"Work_study_hours\", \"Financial Stress\"]\n",
    "for column in columns:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.hist(false_negatives_knn[column], bins=20, alpha=0.5, label='False Negatives', edgecolor=\"red\", linewidth=2, histtype='step', fill=False, density=True)\n",
    "    plt.hist(true_positives[column], bins=20, alpha=0.5, label='True Positives', edgecolor=\"green\", linewidth=2, histtype='step', fill=False,  density=True)\n",
    "    plt.title(f'Distribution of {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main differences between TP and FN are driven by 1) age, 2)academic pressure, 3)study satisfaction, 4)financial stress:\n",
    "- TP are younger, whereas false negatives are in general older;\n",
    "- TP have a right-skewed distribution across academic pressure (more people feeling higher academic pressure)\n",
    "- TP have a left-skewed distribution across study satisfaction (less and less people feeling higher study satisfaction)\n",
    "- TP have a right-skewed distribution across financial stress compared to FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visually comparing TN and FP\n",
    "\n",
    "columns = [\"Age\", \"Academic Pressure\", \"CGPA\", \"Study Satisfaction\", \"Work_study_hours\", \"Financial Stress\"]\n",
    "for column in columns:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.hist(true_negatives_knn[column], bins=20, alpha=0.5, label='True Negative', edgecolor=\"blue\", linewidth=2, histtype='step', fill=False, density=True)\n",
    "    plt.hist(false_positives_knn[column], bins=20, alpha=0.5, label='False Positives', edgecolor=\"orange\", linewidth=2, histtype='step', fill=False,  density=True)\n",
    "    plt.title(f'Distribution of {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main differences between TN and FP are driven by 1) age, 2)academic pressure, 3)study satisfaction, 4)financial stress:\n",
    "- TN are usually older;\n",
    "- TN have a left-skewed distribution across academic pressure (more people feeling less academic pressure);\n",
    "- TN have a right-skewed distribution across study satisfaction (more people feel satisfied by their studies);\n",
    "- TN have a left-skewed distribution across financial stress (non-depressed people are not financially stressed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving the KNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Applying cross-validation (2nd hold-out technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I average the five accuracy values I get for each fold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=2023)\n",
    "scores=cross_val_score(pipeline, X, y, cv=kf, scoring='accuracy')\n",
    "print(f'The accuracy of the model using cross-validation is: {np.mean(scores):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I get a lightly lower accuracy than using train/test split, so this does not improve our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Adding Degree and Cities into the features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_full = df[[\"Gender\",\"City\",\"Degree\",\"Age\",\"Academic Pressure\",\"CGPA\",\"Study Satisfaction\", \"Sleep Duration\",\"Dietary Habits\",\"Suicidal Thoughts\", \"Work_study_hours\",\"Financial Stress\", \"Family History\"]]\n",
    "X_full = pd.get_dummies(columns=[\"Gender\", \"City\",\"Degree\",\"Sleep Duration\", \"Dietary Habits\", \"Suicidal Thoughts\", \"Family History\"], drop_first=True, data=X_full)\n",
    "X_full_train, X_full_test, y_train, y_test = train_test_split(X_full,y, random_state=52, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range=list(range(1,101))\n",
    "training_error=[]\n",
    "testing_error=[]\n",
    "\n",
    "for k in k_range:\n",
    "    pipeline_full = Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier(k))])\n",
    "    pipeline_full.fit(X_full_train, y_train)\n",
    "    \n",
    "    y_pred_knn_full=pipeline_knn_full.predict(X_full_test)\n",
    "    testing_accuracy=metrics.accuracy_score(y_test, y_pred_knn_full)\n",
    "    testing_error.append(1-testing_accuracy)\n",
    "    \n",
    "    y_pred_knn_full=pipeline_knn_full.predict(X_full_train)\n",
    "    training_accuracy=metrics.accuracy_score(y_train, y_pred_knn_full)\n",
    "    training_error.append(1-training_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_knn_full = Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier(94))])\n",
    "pipeline_knn_full.fit(X_full_train, y_train)\n",
    "y_pred_knn_full=pipeline_knn_full.predict(X_full_test)\n",
    "print(f' The accuracy of the KNN model is: {metrics.accuracy_score(y_test, y_pred_knn_full):.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding cities and degrees reduces our accuracy (from 0.85 to 0.67)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Checking the model's best variable combination using SequentialFeatureSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('scaler', StandardScaler()),('knn', KNeighborsClassifier(n_neighbors=88))])\n",
    "sfs = SequentialFeatureSelector(pipeline, n_features_to_select='auto', direction='forward', scoring='accuracy', cv=5, n_jobs=-1)\n",
    "sfs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see which features were selected\n",
    "\n",
    "selected_features = X.columns[sfs.get_support()]\n",
    "print(\"Best feature combination:\", list(selected_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best variable combination: **age, academic pressure, study satisfaction, dietary habits, suicidal thoughts, work study hours and financial stress**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_best = df[[\"Age\",\"Academic Pressure\",\"Study Satisfaction\",\"Dietary Habits\",\"Suicidal Thoughts\", \"Work_study_hours\",\"Financial Stress\"]]\n",
    "X_best = pd.get_dummies(columns=[\"Dietary Habits\", \"Suicidal Thoughts\"], drop_first=True, data=X_best)\n",
    "X_best_train, X_best_test, y_train, y_test = train_test_split(X,y, random_state=52, stratify=y)\n",
    "pipeline_knn_best = Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier(88))])\n",
    "pipeline_knn_best.fit(X_best_train, y_train)\n",
    "y_pred_knn_best=pipeline_knn_best.predict(X_best_test)\n",
    "print(f' The accuracy of the KNN model after selecting the best combination of variables is: {metrics.accuracy_score(y_test, y_pred_knn_best):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_names = ['depressed_no', 'depressed_yes']\n",
    "ConfusionMatrixDisplay.from_estimator(pipeline_knn_best.fit(X_best_train, y_train), X_best_test, y_test,\n",
    "                                 display_labels=class_names, cmap=plt.cm.Blues);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Logistic Regression\n",
    "\n",
    "Logistic Regression is a parametric classification algorithm (unlike KNN), used to predict the probability that an observation belongs to a certain class — typically binary (0 or 1). It ensures that the values output are predictions of class membership that can be interpreted as probabilities. Such probabilities can be converted into class predictions.\n",
    "The logistic regression is modelled as a linear combination of the features: $$\\log \\left({p\\over 1-p}\\right) = \\beta_0 + \\beta_1x$$ This can be rearranged into the **logistic function**: $$p = \\frac{e^{\\beta_0 + \\beta_1x}} {1 + e^{\\beta_0 + \\beta_1x}}$$\n",
    "\n",
    "\n",
    "The logistic regression outperforms KNN, as accuracy here is slightly higher, amounting to 84.5%!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_logreg = Pipeline([('scaler', StandardScaler()), ('logreg', LogisticRegression())])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=51)\n",
    "pipeline_logreg.fit(X_train, y_train)\n",
    "y_pred_logreg = pipeline_logreg.predict(X_test)\n",
    "print(f'The accuracy of the logistic regression is {pipeline_logreg.score(X_test, y_test):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['depressed_no', 'depressed_yes']\n",
    "ConfusionMatrixDisplay.from_estimator(pipeline_logreg, X_test, y_test,\n",
    "                                 display_labels=class_names,\n",
    "                                 cmap=\"Reds\");\n",
    "plt.title(\"Confusion Matrix\");\n",
    "plt.savefig(\"Confusion Matrix from logistic regression\", dpi=300, bbox_inches=\"tight\");\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics.RocCurveDisplay.from_estimator(pipeline_logreg, X_test, y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the coefficients from a pipeline to be able to derive the coefficients\n",
    "\n",
    "logreg = pipeline_logreg.named_steps[\"logreg\"]\n",
    "coefs = logreg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X_train.columns\n",
    "coef_df = pd.DataFrame({'Feature': feature_names,'Magnitude': np.abs(coefs[0])})\n",
    "coef_df.sort_values(by=\"Magnitude\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients that increase the odds, and thus probabilities of an observation belonging to class 1, are, in order of magnitute, **1) suicidal thoughts, 2) academic pressure, 3) financial stress, 4) age, 5) dietary habits**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df.sort_values(by=\"Magnitude\", ascending=False).set_index('Feature')['Magnitude'].plot(kind='bar', figsize=(10,4))\n",
    "plt.title('Coefficient Magnitudes (Logistic Regression)')\n",
    "plt.ylabel('Absolute Value of Coefficient')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SHAP values**\n",
    "\n",
    "SHAP values (short for SHapley Additive exPlanations) tell you how much each feature contributes to a model’s prediction for a given observation, by approximating feature importance through local gradients. They work best with parametric models and tree-based algorithms (logistic/linear regression; tree-based models such as Decision Trees, Random Forests, XGBoost, LightGBM)\n",
    "The explainer is the SHAP object that knows how your model works and computes SHAP values for it. SHAP values are mostly used with tree-based alghorithms.\n",
    "\n",
    "Unlike the gini importance, which is tree-specific, SHAP values investigate how much each feature, for a specific prediction, push the result up or down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(pipeline_logreg.named_steps['logreg'], \n",
    "                           pipeline_logreg.named_steps['scaler'].transform(X))\n",
    "shap_values = explainer(pipeline_logreg.named_steps['scaler'].transform(X))\n",
    "shap_values.feature_names = list(X.columns);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(shap_values.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Waterfall plot**\n",
    "\n",
    "Plot for a single observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.waterfall(shap_values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Absolute mean SHAP plot**\n",
    "\n",
    "Features that have high positive or negative contributions will have large shap values. In this case: suicidal thoughts, academic pressure, financial stress, unhealthy dietary habits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Beeswarm plot**\n",
    "\n",
    "visualisation of all shap values and their direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Decision Tree is a non-parametric supervised machine learning algorithm used for both classification and regression tasks. It models decisions and their possible consequences as a tree-like structure of if-then-else conditions, helping the algorithm learn patterns in the data to make predictions.\n",
    " \n",
    "- The tree starts at the **root node**, which contains the full dataset.\n",
    "- At each node, the algorithm selects the **best feature and threshold** to split the data into two or more branches. The goal is to maximize information gain (in classification) or reduce variance (in regression). For regression, the model picks the feature so that the resulting tree has the **lowest RMSE**. For classification, the model picks the feature that reduces the **Gini index** (from 0 to 0.5 -> when 0, the node is pure).\n",
    "- This splitting process continues recursively until a stopping condition is met (e.g., max depth reached, or nodes are pure).\n",
    "- The final output is a leaf node, which provides the prediction:\n",
    "    - A class label in classification tasks,\n",
    "    - A numerical value in regression tasks.\n",
    "- Decision trees are not sensitive to the scale of the input features, so standardisation/normalisation \n",
    "is not required, unlike for KNN, logistic regression, linear regression, KMeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth_range = list(range(1,200))\n",
    "testing_error=[]\n",
    "training_error=[]\n",
    "\n",
    "for depth in max_depth_range:\n",
    "    treeclf = DecisionTreeClassifier(max_depth=depth, random_state=52)\n",
    "    treeclf.fit(X_train, y_train)\n",
    "    y_pred_class = treeclf.predict(X_test)\n",
    "    testing_accuracy=metrics.accuracy_score(y_test, y_pred_class)\n",
    "    testing_error.append(1-testing_accuracy)    \n",
    "    \n",
    "    y_pred_class = treeclf.predict(X_train)\n",
    "    training_accuracy=metrics.accuracy_score(y_train, y_pred_class)\n",
    "    training_error.append(1-training_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "treeclf_error = pd.DataFrame(list(zip(max_depth_range, training_error, testing_error)), columns=[\"max_length\",\"training_error\",\"testing_error\"])\n",
    "treeclf_error.sort_values(\"testing_error\")\n",
    "\n",
    "#max_lenght=8 leads to the lowest testing error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "treeclf = DecisionTreeClassifier(random_state=1, max_depth=8)\n",
    "treeclf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = treeclf.predict(X_test)\n",
    "accuracy_tree = metrics.accuracy_score(y_test, y_pred)\n",
    "print(f' accuracy: {accuracy_tree:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gini feature importance**\n",
    "\n",
    "Each time a feature is used to split a node in a tree, it causes a reduction in Gini impurity (the Gini index, looking at how mixed the classes at a node are, is minimised). The total reduction in impurity is its Gini importance. A higher Gini importance means that the feature is more influential in making splits that reduce impurity, and therefore in making our model more accurate.\n",
    "\n",
    "Gini importance is specific to trees, and does not mention in what direction the feature affects predictions (negatively or positively). It is also biased towards continuous variables or those with many unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculating Gini importance\n",
    "\n",
    "pd.DataFrame({'feature':X.columns, 'importance':treeclf.feature_importances_}).sort_values(\"importance\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Using the decision tree algorithm, the features with the highest Gini importance are suicidal thoughts, academic pressure, financial stress, age, dietary habits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_graphviz(treeclf, out_file='Decision_Tree', feature_names=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SHAP values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(treeclf)\n",
    "shap_values_dt = explainer(X)\n",
    "shap_values_dt.feature_names = list(X.columns);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(shap_values_dt.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values_dt[:, :, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values_dt[:,:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Random Forest is a supervised machine learning algorithm that consists of a collection (or “ensemble”) of many individual decision trees, typically trained using a method called bagging (each tree is trained on a randomly drawn subset of the training data, to increase diversity). It is used for both classification and regression tasks and is known for its accuracy, robustness, and ability to handle large datasets with high dimensionality.\n",
    "\n",
    "The idea behind a random forest is **to reduce the risk of overfitting** that is often associated with a single decision tree, while maintaining high predictive performance. This is achieved by building many decision trees during training and combining their outputs to make a final prediction:\n",
    "- For **classification**, it predicts the class that is the majority vote among all trees.\n",
    "- For **regression**, it predicts the average of the outputs from all trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=300, max_depth=10,random_state=52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing the best estimator\n",
    "n_estimator_range = range(10, 300, 10)\n",
    "mean_scores = []\n",
    "\n",
    "for n in n_estimator_range:\n",
    "    rf = RandomForestClassifier(n_estimators=n, random_state=52)\n",
    "    scores = cross_val_score(rf, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    mean_scores.append(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(n_estimator_range, mean_scores);\n",
    "plt.xlabel('n_estimators');\n",
    "plt.ylabel('Cross-Validated Accuracy');\n",
    "plt.title('Choosing n_estimators');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting the model\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf=rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#computing accuracy, other metrics and confusion matrix.\n",
    "\n",
    "print(\"Accuracy Random Forest:\", metrics.accuracy_score(y_test, y_pred_rf))\n",
    "print(\"\\nClassification Report:\\n\", metrics.classification_report(y_test, y_pred_rf))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gini feature importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame({\"feature\": X_train.columns,\"importance\": rf_model.feature_importances_\n",
    "}).sort_values(by=\"importance\", ascending=False)\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suicidal Thoughts, Academic Pressure, CGPA, Age, Financial Stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using GridSearch to select the best parameters\n",
    "\n",
    "params = {\"max_depth\": [3, 5, 10, 20, 30], \"n_estimators\": [100, 150, 200, 250, 300]}\n",
    "search = GridSearchCV(RandomForestClassifier(), param_grid=params, cv=5)\n",
    "search.fit(X_train, y_train)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SHAP values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(rf_model)\n",
    "shap_values_rf = explainer(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(shap_values_rf.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values_rf[:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values_rf[:,:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) *for future model improvement* Gradient Boosting Machines (XGBoost, LightGBM, CatBoost)\n",
    "Unfortunately I was not able to import the relevant packages to implement these algorithms. \n",
    "\n",
    "https://medium.com/@weidagang/essential-python-for-machine-learning-xgboost-4b662cf19fcd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What **key factors** affect depression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **KNN**: model accuracy is 0.8405. The best variable combination leading to the highest accuracy is age, academic pressure, study satisfaction, dietary habits, suicidal thoughts, work study hours and financial stress.\n",
    "- **logistic regression**: model accuracy is 0.8455. The strongest coefficients are the ones associated with the following features are suicidal thoughts, academic pressure, financial stress, age, unhealthy dietary habits. Features with large SHAP values include suicidal thoughts, academic pressure, financial stress, unhealthy dietary habits. \n",
    "- **decision tree**: accuracy is 0.8311. The features with the highest Gini importance are suicidal thoughts, academic pressure, financial stress, age, dietary habits.\n",
    "- **random forest**: accuracy is 0.8466. The features with the highest Gini importance are suicidal thoughts,a cademic pressure, CGPA, age, financial stress."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SHAP Env",
   "language": "python",
   "name": "shap_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
